{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40447229-1269-45e4-8f64-bafaba26d399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (0.10.18)\n",
      "Requirement already satisfied: opencv-python in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: absl-py in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (24.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (0.4.35)\n",
      "Requirement already satisfied: jaxlib in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (0.4.35)\n",
      "Requirement already satisfied: matplotlib in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (4.25.5)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.4.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from jax->mediapipe) (0.5.0)\n",
      "Requirement already satisfied: opt-einsum in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from jax->mediapipe) (1.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/sejalnimbargi/.pyenv/versions/3.12.6/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d62a00-2bc3-4f65-8119-deead99bdd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731792723.974628  112751 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 76.3), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1731792724.005877  115597 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731792724.016331  115598 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731792727.008668  115602 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe hands module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Initialize OpenCV to capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for a later selfie-view display\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the frame to RGB (MediaPipe works with RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame to detect hands\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # Draw hand landmarks on the frame\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw landmarks\n",
    "            mp.solutions.drawing_utils.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Hand Tracking\", frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87604221-fa09-4b18-b21c-cfb6f94a4088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/sejalnimbargi/.pyenv/versions/3.12.6/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dac8b19e-eea6-41c5-91b9-25b1ec7b1def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.jpg loaded successfully!\n",
      "B.jpg loaded successfully!\n",
      "C.jpg loaded successfully!\n",
      "D.jpg loaded successfully!\n",
      "E.jpg loaded successfully!\n",
      "F.jpg loaded successfully!\n",
      "G.jpg loaded successfully!\n",
      "H.jpg loaded successfully!\n",
      "I.jpg loaded successfully!\n",
      "J.jpg loaded successfully!\n",
      "K.jpg loaded successfully!\n",
      "L.jpg loaded successfully!\n",
      "M.jpg loaded successfully!\n",
      "N.jpg loaded successfully!\n",
      "O.jpg loaded successfully!\n",
      "P.jpg loaded successfully!\n",
      "Q.jpg loaded successfully!\n",
      "R.jpg loaded successfully!\n",
      "S.jpg loaded successfully!\n",
      "T.jpg loaded successfully!\n",
      "U.jpg loaded successfully!\n",
      "V.jpg loaded successfully!\n",
      "W.jpg loaded successfully!\n",
      "X.jpg loaded successfully!\n",
      "Y.jpg loaded successfully!\n",
      "Z.jpg loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Specify the folder containing ASL images\n",
    "asl_images_path = \"ASL_Letters\"\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# Test loading each image\n",
    "for letter in alphabet:\n",
    "    image_path = os.path.join(asl_images_path, f\"{letter}.jpg\")\n",
    "    if os.path.exists(image_path):\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            print(f\"{letter}.jpg loaded successfully!\")\n",
    "        else:\n",
    "            print(f\"Error: {letter}.jpg exists but cannot be loaded.\")\n",
    "    else:\n",
    "        print(f\"Error: {letter}.jpg not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "142749d1-51e3-4cad-9fec-9cd53978e4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ASL_Letters/A.jpg\n"
     ]
    }
   ],
   "source": [
    "asl_images_path = \"ASL_Letters\"\n",
    "letter = \"A\"\n",
    "\n",
    "# Check both .jpg and .jpeg extensions\n",
    "for ext in [\".jpg\", \".jpeg\"]:\n",
    "    image_path = os.path.join(asl_images_path, letter + ext)\n",
    "    if os.path.exists(image_path):\n",
    "        break\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "if image is not None:\n",
    "    print(f\"Successfully loaded {image_path}\")\n",
    "else:\n",
    "    print(f\"Error: Could not load {letter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4956d0-d281-4ad4-a118-28dfb3be9a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "# Path to the folder with ASL alphabet images\n",
    "asl_images_path = \"ASL_Letters\"\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# Pre-load all ASL images into a dictionary\n",
    "asl_images = {}\n",
    "for letter in alphabet:\n",
    "    asl_images[letter] = cv2.imread(f\"{asl_images_path}/{letter}.jpg\")\n",
    "\n",
    "# Function to animate the ASL translation of a word\n",
    "def animate_asl_word(word):\n",
    "    word = word.upper()  # Ensure the word is in uppercase\n",
    "    for letter in word:\n",
    "        if letter in asl_images:\n",
    "            image = asl_images[letter]\n",
    "            if image is not None:\n",
    "                # Resize the image for display\n",
    "                image = cv2.resize(image, (600, 600))\n",
    "\n",
    "                # Show the image\n",
    "                cv2.imshow(\"ASL Translator\", image)\n",
    "                cv2.waitKey(1000)  # Display each image for 500ms\n",
    "\n",
    "                # Optional: Add a transition effect (e.g., fade-out)\n",
    "                fade_out = cv2.addWeighted(image, 0.5, image, 0.5, 0)\n",
    "                cv2.imshow(\"ASL Translator\", fade_out)\n",
    "                cv2.waitKey(100)  # Add a brief delay for transition effect\n",
    "            else:\n",
    "                print(f\"Image for letter '{letter}' not found!\")\n",
    "        else:\n",
    "            print(f\"Skipping invalid character: {letter}\")\n",
    "\n",
    "    # Wait for the user to close the window\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example\n",
    "animate_asl_word(\"driven\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373368cc-9a89-4000-9a8a-85295cd5015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732379761.104227  730057 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 76.3), renderer: Apple M1\n",
      "W0000 00:00:1732379761.134528  742862 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732379761.144407  742856 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe hand tracking\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# ASL alphabet mapping (simplified example, needs a complete mapping)\n",
    "asl_dict = {\n",
    "    \"A\": [(0.1, 0.2), (0.2, 0.3)],  # Example (you would define landmark coordinates for each letter)\n",
    "    \"B\": [(0.3, 0.4), (0.4, 0.5)],\n",
    "    # Add other letters\n",
    "}\n",
    "\n",
    "def detect_asl_from_landmarks(hand_landmarks):\n",
    "    # Simplified: Map hand landmarks to ASL letters (you need more complex mapping logic)\n",
    "    for letter, landmarks in asl_dict.items():\n",
    "        # Compare hand landmarks with the defined set of landmarks for each letter (example)\n",
    "        if compare_landmarks(hand_landmarks, landmarks):  # You’ll need a function to compare landmarks\n",
    "            return letter\n",
    "    return None\n",
    "\n",
    "def compare_landmarks(hand_landmarks, target_landmarks):\n",
    "    # Placeholder function: compare the hand landmarks with the target landmarks\n",
    "    # You'll need a more sophisticated approach here\n",
    "    return True\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame for selfie-view\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert to RGB as MediaPipe expects RGB input\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame to detect hand landmarks\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw landmarks on the frame\n",
    "            mp.solutions.drawing_utils.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Detect ASL sign from hand landmarks\n",
    "            letter = detect_asl_from_landmarks(hand_landmarks)\n",
    "            if letter:\n",
    "                print(f\"Detected Letter: {letter}\")  # Display the recognized letter\n",
    "\n",
    "    # Show the video feed with hand landmarks\n",
    "    cv2.imshow(\"Hand Tracking\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59f1c400-c40c-47ea-bec7-962722d3f91a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: mediapipe in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (0.10.18)\n",
      "Requirement already satisfied: opencv-python in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (24.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (0.4.35)\n",
      "Requirement already satisfied: jaxlib in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (0.4.35)\n",
      "Requirement already satisfied: matplotlib in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (4.25.5)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.4.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from jax->mediapipe) (0.5.0)\n",
      "Requirement already satisfied: opt-einsum in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sejalnimbargi/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/sejalnimbargi/.pyenv/versions/3.12.6/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6fb2d-7b44-4ef0-84a2-83d13272a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pretrained Random Forest model\n",
    "with open(\"best_random_forest_model.pkl\", \"rb\") as model_file:\n",
    "    rf_model = pickle.load(model_file)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91bc505f-24a5-4dda-b83f-c5b092b4919c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732385930.824312  802765 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 76.3), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1732385930.853004  802885 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732385930.870860  802886 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732385933.108481  802887 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved A image!\n",
      "Saved B image!\n",
      "Saved C image!\n",
      "Saved D image!\n",
      "Saved E image!\n",
      "Saved F image!\n",
      "Saved G image!\n",
      "Saved H image!\n",
      "Saved I image!\n",
      "Saved J image!\n",
      "Saved K image!\n",
      "Saved L image!\n",
      "Saved M image!\n",
      "Saved N image!\n",
      "Saved O image!\n",
      "Saved P image!\n",
      "Saved Q image!\n",
      "Saved R image!\n",
      "Saved S image!\n",
      "Saved T image!\n",
      "Saved U image!\n",
      "Saved V image!\n",
      "Saved W image!\n",
      "Saved X image!\n",
      "Saved Y image!\n",
      "Saved Z image!\n",
      "Captured all gestures from A to Z!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "# Set up directories to save images\n",
    "image_dir = \"asl_images\"\n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir)\n",
    "\n",
    "# Initialize MediaPipe Hands model\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Initialize OpenCV for video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Alphabet letters for ASL (A-Z)\n",
    "letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "letter_index = 0  # Keep track of the current letter\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Flip the frame horizontally for a better selfie view\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the frame to RGB (MediaPipe works in RGB)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame for hand landmarks\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # Draw hand landmarks if detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp.solutions.drawing_utils.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display the current letter to capture\n",
    "    cv2.putText(frame, f\"Press 's' to save image for: {letters[letter_index]}\", (50, 50), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame with hand landmarks\n",
    "    cv2.imshow(\"Hand Gesture Capture\", frame)\n",
    "\n",
    "    # Wait for key press\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # Press 's' to save the image for the current letter\n",
    "    if key == ord('s'):\n",
    "        image_filename = f\"{image_dir}/{letters[letter_index]}.jpg\"\n",
    "        cv2.imwrite(image_filename, frame)  # Save the captured image\n",
    "        print(f\"Saved {letters[letter_index]} image!\")\n",
    "        \n",
    "        # Display a confirmation message on screen\n",
    "        cv2.putText(frame, f\"Image for {letters[letter_index]} saved!\", (50, 100), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "        # Display the saved message for 2 seconds\n",
    "        cv2.imshow(\"Hand Gesture Capture\", frame)\n",
    "        cv2.waitKey(2000)  # Show message for 2 seconds\n",
    "\n",
    "        # Move to the next letter (A-Z)\n",
    "        letter_index += 1\n",
    "\n",
    "        # If we've reached Z, exit the loop\n",
    "        if letter_index >= len(letters):\n",
    "            print(\"Captured all gestures from A to Z!\")\n",
    "            break\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5616a37-66a7-4bcc-b1c9-f45137e34b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733596076.746900   44583 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 76.3), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733596076.768545   61530 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733596076.780885   61531 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733596076.798370   61530 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os  # Ensure os is imported!\n",
    "\n",
    "# Initialize MediaPipe hands model\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-captured ASL images and prepare feature mappings\n",
    "asl_images_path = \"asl_images\"\n",
    "letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# Create a dictionary to hold letter mappings\n",
    "asl_landmark_features = {}\n",
    "\n",
    "def extract_landmark_features(image_path):\n",
    "    \"\"\"Extract landmark positions from an image.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Extract landmarks into a list of tuples\n",
    "            return [(lm.x, lm.y) for lm in hand_landmarks.landmark]\n",
    "    return None\n",
    "\n",
    "# Preload ASL landmark features for A-Z\n",
    "for letter in letters:\n",
    "    image_path = f\"{asl_images_path}/{letter}.jpg\"\n",
    "    if os.path.exists(image_path):  # Check for file existence\n",
    "        features = extract_landmark_features(image_path)\n",
    "        if features:\n",
    "            asl_landmark_features[letter] = features\n",
    "\n",
    "def calculate_distance(landmark_set1, landmark_set2):\n",
    "    \"\"\"Calculate Euclidean distance between two sets of landmarks.\"\"\"\n",
    "    if len(landmark_set1) != len(landmark_set2):\n",
    "        return float('inf')  # Return a high value if sets are mismatched\n",
    "    return np.sqrt(np.sum((np.array(landmark_set1) - np.array(landmark_set2))**2))\n",
    "\n",
    "def predict_asl_letter(hand_landmarks):\n",
    "    \"\"\"Predict the ASL letter based on input hand landmarks.\"\"\"\n",
    "    hand_features = [(lm.x, lm.y) for lm in hand_landmarks.landmark]\n",
    "    predicted_letter = None\n",
    "    min_distance = float('inf')\n",
    "\n",
    "    for letter, features in asl_landmark_features.items():\n",
    "        distance = calculate_distance(hand_features, features)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            predicted_letter = letter\n",
    "\n",
    "    return predicted_letter\n",
    "\n",
    "# Initialize OpenCV for video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame for selfie-view\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the frame to RGB as MediaPipe expects RGB input\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame to detect hand landmarks\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    detected_letter = None\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw landmarks on the frame\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Predict ASL letter from detected hand landmarks\n",
    "            detected_letter = predict_asl_letter(hand_landmarks)\n",
    "\n",
    "    # Display detected letter on the frame\n",
    "    if detected_letter:\n",
    "        cv2.putText(frame, f\"Detected: {detected_letter}\", (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the video feed with detection\n",
    "    cv2.imshow(\"ASL Translator\", frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707e389-d10f-449c-87be-b66c1e8b4628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
